{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "This notebook will demonstate how to automatically tune model hyperparameters for prediction accuracy.\n",
    "\n",
    "## Set-Up\n",
    "We will begin with some imports and then generate some training data using the simple cart-pole benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AutoMPC...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/.local/lib/python3.8/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading AutoMPC\n"
     ]
    }
   ],
   "source": [
    "import autompc as ampc\n",
    "import numpy as np\n",
    "\n",
    "from autompc.benchmarks.mcube_pushing import MCubePushingBenchmark\n",
    "\n",
    "benchmark = MCubePushingBenchmark()\n",
    "\n",
    "system = benchmark.system\n",
    "trajs = benchmark.get_trajs(num_trajs=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "191"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trajs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.005"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system.dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/william/proj/autompc_0.2-dev/autompc/__init__.py'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ampc.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, AutoMPC's `ModelTuner` will auto-select from all available models to fit the trajectories as best as possible. The tuner by default will run for 10 iterations, but for real problems you will want to run for many, many more iterations (100s)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding unsupported scenario options: {'save_results_instantly': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda=True\n",
      "torch.cuda.is_available()=False\n",
      "self._device=device(type='cpu')\n",
      "Foo\n",
      "Selecting from models MLP,ARX,Koopman,SINDy,ApproximateGPModel\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Exception during evaluation\n",
      "Exit code:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Target Algorithm returned NaN or inf as quality. Algorithm run is treated as CRASHED, cost is set to 2147483647.0 for quality scenarios. (Change value through \"cost_for_crash\"-option.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CfgRunner: Evaluation timed out\n",
      "Selected model: ARX\n",
      "Final cross-validated RMSE score: 0.054394540180323046\n"
     ]
    }
   ],
   "source": [
    "from autompc.tuning import ModelTuner\n",
    "\n",
    "tuner = ModelTuner(system,trajs,verbose=1)\n",
    "print(\"Selecting from models\",\",\".join(model.name for model in tuner.model.models))\n",
    "tuned_model,tune_result = tuner.run(n_iters=100, eval_timeout=600)\n",
    "\n",
    "print(\"Selected model:\",tuned_model.name)\n",
    "print(\"Final cross-validated RMSE score:\",tune_result.inc_costs[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tune_result' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e750705e8b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtune_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc_costs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tune_result' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can customize the behavior of tuning by specifying which evaluation strategy we wish to use.  Possible options include splitting method (holdout vs cross-validation), which horizon to measure predictions upon, and what scoring metric to use.  By default, ModelTuner uses 3-fold cross-validation and one-step RMSE.\n",
    "\n",
    "Here's an example of customizing the evaluator to use with 10\\% holdout and the RMSE metric with a 5-step prediction horizon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_cuda=True\n",
      "torch.cuda.is_available()=False\n",
      "self._device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "tuner = ModelTuner(system,trajs,eval_holdout=0.1,eval_folds=1,eval_metric='rmse',eval_horizon=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To customize behavior even further, we can use a `ModelEvaluator` class, which has various subclasses.  As an example, the HoldoutModelEvaluator is specified here.  The `evaluator` keyword to ModelTuner will specify an evaluator that overrides the default keyword arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'HoldoutModelEvaluator' from 'autompc.tuning' (/home/william/proj/autompc_0.2-dev/autompc/tuning/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bb181575b415>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautompc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHoldoutModelEvaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m evaluator = HoldoutModelEvaluator(trajs, metric=\"rmse\", holdout_prop=0.1,\n\u001b[1;32m      4\u001b[0m                                   rng=np.random.default_rng(100), horizon=20)\n\u001b[1;32m      5\u001b[0m \u001b[0mtuner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModelTuner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrajs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'HoldoutModelEvaluator' from 'autompc.tuning' (/home/william/proj/autompc_0.2-dev/autompc/tuning/__init__.py)"
     ]
    }
   ],
   "source": [
    "from autompc.tuning import HoldoutModelEvaluator\n",
    "\n",
    "evaluator = HoldoutModelEvaluator(trajs, metric=\"rmse\", holdout_prop=0.1,\n",
    "                                  rng=np.random.default_rng(100), horizon=20)\n",
    "tuner = ModelTuner(system,trajs,evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying the Model Class\n",
    "\n",
    "In some cases we know which model class we wish to tune, and ModelTuner can also accept a specified class.  For example, we can consider the MLP model. Here we'll perform a much longer tuning run, so let this run for a few hours...\n",
    "\n",
    "**Alternatively, you can save/load the tuning data from a prior run by setting `dump=False` in the following cell, and skipping the tuning altogether.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from autompc.sysid import MLP\n",
    "\n",
    "tuner = ModelTuner(system,trajs,MLP(system),verbose=True)\n",
    "tuned_model, tune_result = tuner.run(n_iters=200,rng=np.random.default_rng(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#To dump tuning result, turn to True.  To load it, turn to False\n",
    "DUMP = False\n",
    "if DUMP:\n",
    "    with open('tuned_mlp_model.pkl','wb') as f:\n",
    "        pickle.dump(tuned_model,f)\n",
    "    with open('mlp_model_tuning_result.pkl','wb') as f:\n",
    "        pickle.dump(tune_result,f)\n",
    "else:\n",
    "    with open('tuned_mlp_model.pkl','rb') as f:\n",
    "        tuned_model = pickle.load(f)\n",
    "    with open('mlp_model_tuning_result.pkl','rb') as f:\n",
    "        tune_result = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what configuration was selected by the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_result.inc_cfg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the tuner selected a 2-layer MLP with `tanh` activations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's note that another option is to specify a set of model classes to use. To do so we can use the AutoSelectModel class as the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autompc.sysid import SINDy\n",
    "from autompc.sysid import AutoSelectModel\n",
    "\n",
    "selector = AutoSelectModel(system,[MLP(system),SINDy(system)])\n",
    "tuner = ModelTuner(system,trajs,model=selector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Results\n",
    "\n",
    "We can now visualize the tuning results.  First, we will plot the tuning curve.  This shows the holdout set performance of the best-known model at different points over the course of the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'ModelTuneResult' from 'autompc.tuning.model_tuner' (/home/william/proj/autompc_0.2-dev/autompc/tuning/model_tuner.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8498594d5481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautompc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_tuning_curve\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_tuning_correlations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/proj/autompc_0.2-dev/autompc/graphs/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mkstep_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKstepPredAccGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtuning_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_tuning_curve\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_tuning_correlations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtrajectory_graph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_traj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_traj_projected\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_rollout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_simulation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_trajs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplot_trajs_projected\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/proj/autompc_0.2-dev/autompc/graphs/tuning_graph.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_tuner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mControlTunerResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtuning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_tuner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelTuneResult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_tuning_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'ModelTuneResult' from 'autompc.tuning.model_tuner' (/home/william/proj/autompc_0.2-dev/autompc/tuning/model_tuner.py)"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from autompc.graphs import plot_tuning_curve,plot_tuning_correlations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_tuning_curve(tune_result)\n",
    "plt.title(\"MCube Pushing Tuning Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_result.cfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"2022-09-25_cartpole_tune_result.pkl\", \"wb\") as f:\n",
    "    pickle.dump(tune_result, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further study the tuning, we can examine how the cost correlates with various hyperparameter settings using the `plot_tuning_correlations` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,(ax1,ax2,ax3)=plt.subplots(1,3,figsize=(15,4))\n",
    "plot_tuning_correlations(tune_result,'lr',ax=ax1)\n",
    "plot_tuning_correlations(tune_result,'n_hidden_layers',ax=ax2)\n",
    "plot_tuning_correlations(tune_result,'nonlintype',ax=ax3)\n",
    "ax1.set_ylim(0,1.5)\n",
    "ax2.set_ylim(0,1.5)\n",
    "ax3.set_ylim(0,3.0)\n",
    "ax3.set_title('nonlintype')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can compare the performance of our tuned model to the default MLP configuration.  We will generate a fresh dataset for testing and compare over multiple prediction horizons.  For more details on how to do this comparison, see [2. Models]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untuned_model = MLP(system)\n",
    "untuned_model.train(trajs)\n",
    "\n",
    "testing_set = benchmark.gen_trajs(seed=101, n_trajs=100, traj_len=200)\n",
    "\n",
    "from autompc.graphs.kstep_graph import KstepPredAccGraph\n",
    "\n",
    "graph = KstepPredAccGraph(system, testing_set, kmax=20, metric=\"rmse\")\n",
    "graph.add_model(untuned_model, \"Untuned MLP\")\n",
    "graph.add_model(tuned_model, \"Tuned Model (MLP)\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "graph(fig, ax)\n",
    "ax.set_title(\"Model prediction accuracy on test set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the tuned model outperforms the untuned model on the unseen dataset at all prediction horizons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = KstepPredAccGraph(system, trajs, kmax=20, metric=\"rmse\")\n",
    "graph.add_model(untuned_model, \"Untuned MLP\")\n",
    "graph.add_model(tuned_model, \"Tuned Model (MLP)\")\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "graph(fig, ax)\n",
    "ax.set_title(\"Model prediction accuracy on training set\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
