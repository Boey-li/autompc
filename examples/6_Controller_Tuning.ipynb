{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Tuning\n",
    "\n",
    "This notebook will demonstate how to automatically tune model hyperparameters for predictions accuracy.\n",
    "\n",
    "## Set-Up\n",
    "We will begin with some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading AutoMPC...\n",
      "Finished loading AutoMPC\n"
     ]
    }
   ],
   "source": [
    "import autompc as ampc\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we will then generate some training data using the benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autompc.benchmarks import CartpoleSwingupV2Benchmark\n",
    "\n",
    "benchmark = CartpoleSwingupV2Benchmark()\n",
    "\n",
    "system = benchmark.system\n",
    "trajs = benchmark.gen_trajs(seed=100, n_trajs=500, traj_len=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Task\n",
    "\n",
    "The first things we need to do is create a Task.  The Task object encapsulates all the information the tuner needs to evaluate controllers.  The Task includes an OCP, as well as additional information such as the initial and termination conditions.\n",
    "\n",
    "Let's create an OCP using the ThresholdCost and add relevant control bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autompc.costs import ThresholdCost\n",
    "\n",
    "ocp = ampc.OCP(system)\n",
    "ocp.set_cost(ThresholdCost(system, goal=np.zeros(system.obs_dim), threshold=0.2, observations=[\"theta\", \"omega\"]))\n",
    "ocp.set_ctrl_bound(\"u\", -20.0, 20.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we create the Task, associating it with the OCP and other relevant information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ampc.Task(system)\n",
    "task.set_ocp(ocp)\n",
    "task.set_init_obs(np.array([np.pi, 0.0, 0.0, 0.0]))\n",
    "task.set_num_steps(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up a Controller\n",
    "\n",
    "First, we need to create the controller to be tuned.  We can give the controller several options for the model and optimizer.  The tuner will automatically select between these.  We also add an OCP Factory, which allows the tuner to construct a new OCP with tuned weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'QuadCostFactory' from 'autompc.ocp' (/home/dohun/Repos/autompc/autompc/ocp/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/home/dohun/Repos/autompc/examples/6_Controller_Tuning.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dohun/Repos/autompc/examples/6_Controller_Tuning.ipynb#ch0000009?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautompc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msysid\u001b[39;00m \u001b[39mimport\u001b[39;00m MLP, SINDy\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dohun/Repos/autompc/examples/6_Controller_Tuning.ipynb#ch0000009?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautompc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39moptim\u001b[39;00m \u001b[39mimport\u001b[39;00m IterativeLQR, MPPI\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/dohun/Repos/autompc/examples/6_Controller_Tuning.ipynb#ch0000009?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mautompc\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mocp\u001b[39;00m \u001b[39mimport\u001b[39;00m QuadCostFactory\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dohun/Repos/autompc/examples/6_Controller_Tuning.ipynb#ch0000009?line=4'>5</a>\u001b[0m controller \u001b[39m=\u001b[39m ampc\u001b[39m.\u001b[39mController(system)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dohun/Repos/autompc/examples/6_Controller_Tuning.ipynb#ch0000009?line=5'>6</a>\u001b[0m controller\u001b[39m.\u001b[39madd_model(MLP(system))\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'QuadCostFactory' from 'autompc.ocp' (/home/dohun/Repos/autompc/autompc/ocp/__init__.py)"
     ]
    }
   ],
   "source": [
    "from autompc.sysid import MLP, SINDy\n",
    "from autompc.optim import IterativeLQR, MPPI\n",
    "from autompc.ocp import QuadCostTransformer\n",
    "\n",
    "controller = ampc.Controller(system)\n",
    "controller.add_model(MLP(system, n_train_iters=10))\n",
    "# controller.add_model(SINDy(system))\n",
    "controller.add_optimizer(IterativeLQR(system))\n",
    "# controller.add_optimizer(MPPI(system))\n",
    "controller.add_ocp_transformer(QuadCostTransformer(system))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this gives a joint configuration space over all algorithm options.  This defines the search space for the tuner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Configuration space object:\n",
       "  Hyperparameters:\n",
       "    IterativeLQR:frequency, Type: UniformInteger, Range: [1, 5], Default: 1\n",
       "    IterativeLQR:horizon, Type: UniformInteger, Range: [5, 25], Default: 20\n",
       "    IterativeLQR:max_iter, Type: UniformInteger, Range: [10, 50], Default: 20\n",
       "    MLP:batchnorm, Type: Categorical, Choices: {False, True}, Default: False\n",
       "    MLP:hidden_size_1, Type: UniformInteger, Range: [16, 256], Default: 128\n",
       "    MLP:hidden_size_2, Type: UniformInteger, Range: [16, 256], Default: 128\n",
       "    MLP:hidden_size_3, Type: UniformInteger, Range: [16, 256], Default: 128\n",
       "    MLP:hidden_size_4, Type: UniformInteger, Range: [16, 256], Default: 128\n",
       "    MLP:lr, Type: UniformFloat, Range: [1e-05, 1.0], Default: 0.001, on log-scale\n",
       "    MLP:n_hidden_layers, Type: Categorical, Choices: {1, 2, 3, 4}, Default: 2\n",
       "    MLP:nonlintype, Type: Categorical, Choices: {relu, tanh, sigmoid, selu}, Default: relu\n",
       "    QuadCostTransformer:dx_F, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:dx_Q, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:omega_F, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:omega_Q, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:theta_F, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:theta_Q, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:u_R, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:x_F, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    QuadCostTransformer:x_Q, Type: UniformFloat, Range: [0.001, 10000.0], Default: 1.0, on log-scale\n",
       "    constraint_transformer, Type: Categorical, Choices: {_}, Default: _\n",
       "    cost_transformer, Type: Categorical, Choices: {QuadCostTransformer, _}, Default: QuadCostTransformer\n",
       "    model, Type: Categorical, Choices: {MLP}, Default: MLP\n",
       "    optimizer, Type: Categorical, Choices: {IterativeLQR}, Default: IterativeLQR\n",
       "  Conditions:\n",
       "    IterativeLQR:frequency | optimizer == 'IterativeLQR'\n",
       "    IterativeLQR:horizon | optimizer == 'IterativeLQR'\n",
       "    IterativeLQR:max_iter | optimizer == 'IterativeLQR'\n",
       "    MLP:batchnorm | model == 'MLP'\n",
       "    MLP:hidden_size_1 | model == 'MLP'\n",
       "    MLP:hidden_size_2 | MLP:n_hidden_layers in {'2', '3', '4'}\n",
       "    MLP:hidden_size_3 | MLP:n_hidden_layers in {'3', '4'}\n",
       "    MLP:hidden_size_4 | MLP:n_hidden_layers in {'4'}\n",
       "    MLP:lr | model == 'MLP'\n",
       "    MLP:n_hidden_layers | model == 'MLP'\n",
       "    MLP:nonlintype | model == 'MLP'\n",
       "    QuadCostTransformer:dx_F | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:dx_Q | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:omega_F | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:omega_Q | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:theta_F | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:theta_Q | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:u_R | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:x_F | cost_transformer == 'QuadCostTransformer'\n",
       "    QuadCostTransformer:x_Q | cost_transformer == 'QuadCostTransformer'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controller.get_config_space()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Tuner\n",
    "\n",
    "Next, we will create a `ControlTuner`, which will search the joint controller configuration space.  The `ControlTuner` evaluates controller configurations by simulating them against a learned surrogate model.  There are a few options we can choose for how to construct the surrogate model.  Here we set the surrogate mode to `default`, which means the `ControlTuner` will learn the surrogate model according to the default configuration of a given model.  We then pass an `MLP` instance as the surrogate model to be trained and set the `surrogate_split` to 0.5.  Which means half the trajectories will be used for training the surrogate model, and half for training the controller.  Finally, we specify an `output_dir` where AutoMPC will store intermediate and output files.  If the output directory is not specified, AutoMPC will automatically create one in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autompc.tuning import ControlTuner\n",
    "from autompc.sysid import MLP\n",
    "\n",
    "surrogate = MLP(system, n_train_iters=10)\n",
    "surrogate.freeze_hyperparameters()\n",
    "\n",
    "tuner = ControlTuner(surrogate=surrogate, surrogate_split=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "surrogate.is_tunable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important surrogate mode to be aware of is `autoselect`.  This mode will automatically tune and select the surrogate model before tuning the controller.  This is a great option if you are not sure which model will work well, but it will take significantly longer to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Tuner\n",
    "\n",
    "Now, we can run the tuner.  We pass the `Controller` to be tuned, the `Task` which specifies the tuning objective, and the trajectory set, which will be randomly partitioned into a surrogate training set and a controller training set.  Additional arguments include\n",
    " * **n_iters** : The number of configurations to evalute during tuning.\n",
    " * **rng** : Numpy random number generator\n",
    " * **output_dir** : AutoMPC output and intermediary files are stored here.  If not specified, a directory is automatically created in the current workdir.\n",
    " * **truedyn** : For benchmarking purposes, we can pass a function which computes the true dynamics, so we can evalaute the accuracy of the surrogate model in measuring controller performance.\n",
    " \n",
    " The tuner returns the tuned controller, and a `ControlTunerResult` instance which stores information about the tuning run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/william/.local/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding unsupported scenario options: {'save_results_instantly': True}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Checking compatibility with OCP\n",
      "Requiring cost transformer for ocp to be used with IterativeLQR due to property is_twice_diff\n",
      "100%|██████████| 10/10 [00:10<00:00,  1.01s/it]\n"
     ]
    }
   ],
   "source": [
    "tuned_controller, tune_result = tuner.run(\n",
    "    controller,\n",
    "    task,\n",
    "    trajs,\n",
    "    n_iters=3,\n",
    "    rng=np.random.default_rng(100),\n",
    "    output_dir=\"6_controller_tuning_example_output/\",\n",
    "    truedyn=benchmark.dynamics\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Tuning Results\n",
    "\n",
    "AutoMPC provides tools based on matplotlib to visualize the results of a tuning run.  Here, we visualize the tuning curve, which shows the surrogate-estimated performance of best controller found after each iteration of tuning.  When the `truedyn` argument is provided to the tuner, this graph also plots the true dynamics performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.000000000000007, 10.000000000000007, 10.000000000000007]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tune_result.costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TuningCurveGraph' from 'autompc.graphs' (/home/william/proj/autompc_multitask/autompc/graphs/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c5a177c139e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mautompc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraphs\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTuningCurveGraph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTuningCurveGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TuningCurveGraph' from 'autompc.graphs' (/home/william/proj/autompc_multitask/autompc/graphs/__init__.py)"
     ]
    }
   ],
   "source": [
    "from autompc.graphs import TuningCurveGraph\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "graph = TuningCurveGraph()\n",
    "\n",
    "fig = plt.figure()      \n",
    "ax = fig.gca()\n",
    "graph(ax, tune_result)\n",
    "ax.set_title(\"Cart-Pole Tuning Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further analyze the results of a tuning run, we can inspect the fields of the `ControlTuneResult` object.  Key fields include\n",
    " * **inc_cfg** : The final incumbent (selected) configuration.\n",
    " * **inc_cfgs** : A list of the incumbent configurations at each iteration.\n",
    " * **inc_costs** : The incumbent cost at each iteration.\n",
    " * **cfgs** : A list of the configurations evaluated at each iteration.\n",
    " * **costs** : A list of the costs of configruations evaluated at each iteration.\n",
    "\n",
    "For more detailed documentation, see the API reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resuming a Tuning Run\n",
    "\n",
    "If a tuning run is interrupted partway through, you can resume by calling `tuner.run` with the keyword argument `restore_dir` containing the output directory of the original run.  The feature can also be used to increase or decrease the tuning iterations of a tuning run that is already in progress."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": false,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "8205f83729152a74c9afc694ae4592a9e3038e45d9cbfebf1ba348127e2980f0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
